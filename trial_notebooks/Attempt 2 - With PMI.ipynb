{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_fname=\"/home/hank/Backup_data/data/finalex_reuters-cleaned-document_without_zeros.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading documents & Calculating term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " inserting ... 203001 docs, mem= 2.509 Gb"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import sys\n",
    "from utils import get_process_memory\n",
    "\n",
    "M_term_doc = defaultdict(lambda: {})\n",
    "with open(document_fname, encoding='utf-8') as f:\n",
    "    for d, doc in enumerate(f):\n",
    "        tf = Counter(doc.split())\n",
    "        for t, freq in tf.items():\n",
    "            M_term_doc[t][d] = freq\n",
    "        if d % 1000 == 0: \n",
    "            sys.stdout.write('\\r inserting ... %d docs, mem= %.3f Gb' %(d+1, get_process_memory()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing BOC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d200_w8_mf50_c200.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "#word2concept_fname=[ef for ef in glob.glob(\"/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c*.csv\")]\n",
    "word2concept_fname=['/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d200_w8_mf50_c200.csv']\n",
    "print(word2concept_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for calculating co-occurence between two terms within same document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cooccurrence(w1, w2):\n",
    "    docs1 = M_term_doc.get(w1, {})\n",
    "    docs2 = M_term_doc.get(w2, {})\n",
    "    cooccurrence = 0\n",
    "    for d1, tf_d1w1 in docs1.items():\n",
    "        tf_d1w2 = docs2.get(d1, 0)\n",
    "        if not tf_d1w2:\n",
    "            continue\n",
    "        cooccurrence += max(tf_d1w1, tf_d1w2)\n",
    "    return cooccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_tf = lambda w:sum(M_term_doc[w].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively calculate PMI score for words within their respective concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2c_d200_w8_mf50_c200pmi.csv\n",
      "done. mem= 2.754 Gb\n",
      "done. mem= 2.754 Gb\n",
      "done. mem= 2.754 Gb\n",
      "done. mem= 2.754 Gb\n",
      "done. mem= 2.754 Gb\n",
      "done. mem= 2.754 Gb\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e192f028e427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mcooc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcooccurrence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mM_cooccurrence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcooc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mM_cooccurrence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcooc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b00a7538f396>\u001b[0m in \u001b[0;36mcooccurrence\u001b[0;34m(w1, w2)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdocs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_term_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcooccurrence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_d1w1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtf_d1w2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf_d1w2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for es in word2concept_fname:\n",
    "    outputname=es.split(\"/\")[-1][:-4]+\"_pmi.csv\"\n",
    "    print(outputname)\n",
    "    concept_to_words = defaultdict(lambda: [])\n",
    "    with open(es, encoding='utf-8') as f:\n",
    "        for row in f:\n",
    "            cols = row.strip().split(',')\n",
    "            concept = int(cols[-1])\n",
    "            words = ','.join(cols[:-1])\n",
    "            concept_to_words[concept].append(words)\n",
    "    M_cooccurrence = defaultdict(lambda: {})\n",
    "    for concept, words in concept_to_words.items():\n",
    "        #print('concept= %d (%d words) ... ' % (concept, len(words)), end='')\n",
    "        for w1 in words:\n",
    "            for w2 in words:\n",
    "                if w1 <= w2: continue\n",
    "                cooc = cooccurrence(w1, w2)\n",
    "                M_cooccurrence[w1][w2] = cooc\n",
    "                M_cooccurrence[w2][w1] = cooc\n",
    "    print('done. mem= %.3f Gb' % get_process_memory())\n",
    "    word_to_pmi = {}\n",
    "\n",
    "    m = 1\n",
    "    n = 2\n",
    "\n",
    "    i_words = 0\n",
    "    n_words = sum((len(words) for words in concept_to_words.values()))\n",
    "\n",
    "    for concept, words in concept_to_words.items():\n",
    "        for word in words:\n",
    "            pmi=0\n",
    "            i_words += 1\n",
    "            if i_words % 100 == 0:\n",
    "                args = (i_words, n_words, get_process_memory())\n",
    "                sys.stdout.write('\\r computing sparsity ... %d words in %d. mem= %.3f Gb' % args)\n",
    "\n",
    "            cooccurrence_vector = M_cooccurrence.get(word, {})\n",
    "\n",
    "            if not cooccurrence_vector:\n",
    "                continue\n",
    "            \n",
    "            for word2, cooc in cooccurrence_vector:\n",
    "                pmi+=math.log(cooc/(get_tf(word)*get_tf(word2)))\n",
    "            \n",
    "            word_to_pmi[word]=pmi\n",
    "            \n",
    "    print('\\ndone')\n",
    "    \n",
    "    with open(outputname, \"w\") as f:\n",
    "        for concept, words in concept_to_words.items():\n",
    "            topk_words = sorted(words, key=lambda x:word_to_pmi[x])\n",
    "            for w in topk_words:\n",
    "                f.write('%d, %s, %.3f, %d, %d\\n' % (concept, w, word_to_pmi(w), get_tf(w), get_df(w)))\n",
    "    print('....%s created' % outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
