{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_fname=\"change needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading documents & Calculating term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " inserting ... 203001 docs, mem= 2.509 Gb"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import sys\n",
    "from utils import get_process_memory\n",
    "\n",
    "M_term_doc = defaultdict(lambda: {})\n",
    "with open(document_fname, encoding='utf-8') as f:\n",
    "    for d, doc in enumerate(f):\n",
    "        tf = Counter(doc.split())\n",
    "        for t, freq in tf.items():\n",
    "            M_term_doc[t][d] = freq\n",
    "        if d % 1000 == 0: \n",
    "            sys.stdout.write('\\r inserting ... %d docs, mem= %.3f Gb' %(d+1, get_process_memory()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing BOC models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d300_w8_mf50_c100.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d200_w8_mf50_c200.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d300_w8_mf50_c300.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d200_w8_mf50_c100.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d100_w8_mf50_c300.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d200_w8_mf50_c300.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d100_w8_mf50_c100.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d100_w8_mf50_c200.csv', '/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d300_w8_mf50_c200.csv']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "word2concept_fname=[ef for ef in glob.glob(\"/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c*.csv\")]\n",
    "#word2concept_fname=['/home/hank/Desktop/projects/auto_concept_labeling_POC/trained_results/w2c_d200_w8_mf50_c200.csv']\n",
    "print(word2concept_fname)\n",
    "print(len(word2concept_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for calculating co-occurence between two terms within same document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cooccurrence(w1, w2):\n",
    "    docs1 = M_term_doc.get(w1, {})\n",
    "    docs2 = M_term_doc.get(w2, {})\n",
    "    cooccurrence = 0\n",
    "    for d1, tf_d1w1 in docs1.items():\n",
    "        tf_d1w2 = docs2.get(d1, 0)\n",
    "        if not tf_d1w2:\n",
    "            continue\n",
    "        cooccurrence += max(tf_d1w1, tf_d1w2)\n",
    "    return cooccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_tf = lambda w:sum(M_term_doc[w].values())\n",
    "get_df = lambda w:len(M_term_doc[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively calculate PMI score for words within their respective concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2c_d300_w8_mf50_c100_pmi.csv\n",
      "done. mem= 3.393 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.394 Gb\n",
      "done\n",
      "....w2c_d300_w8_mf50_c100_pmi.csv created\n",
      "w2c_d200_w8_mf50_c200_pmi.csv\n",
      "done. mem= 3.392 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.392 Gb\n",
      "done\n",
      "....w2c_d200_w8_mf50_c200_pmi.csv created\n",
      "w2c_d300_w8_mf50_c300_pmi.csv\n",
      "done. mem= 2.949 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 2.949 Gb\n",
      "done\n",
      "....w2c_d300_w8_mf50_c300_pmi.csv created\n",
      "w2c_d200_w8_mf50_c100_pmi.csv\n",
      "done. mem= 3.408 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.410 Gb\n",
      "done\n",
      "....w2c_d200_w8_mf50_c100_pmi.csv created\n",
      "w2c_d100_w8_mf50_c300_pmi.csv\n",
      "done. mem= 3.409 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.409 Gb\n",
      "done\n",
      "....w2c_d100_w8_mf50_c300_pmi.csv created\n",
      "w2c_d200_w8_mf50_c300_pmi.csv\n",
      "done. mem= 3.406 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.406 Gb\n",
      "done\n",
      "....w2c_d200_w8_mf50_c300_pmi.csv created\n",
      "w2c_d100_w8_mf50_c100_pmi.csv\n",
      "done. mem= 3.390 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.392 Gb\n",
      "done\n",
      "....w2c_d100_w8_mf50_c100_pmi.csv created\n",
      "w2c_d100_w8_mf50_c200_pmi.csv\n",
      "done. mem= 3.389 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.389 Gb\n",
      "done\n",
      "....w2c_d100_w8_mf50_c200_pmi.csv created\n",
      "w2c_d300_w8_mf50_c200_pmi.csv\n",
      "done. mem= 3.284 Gb\n",
      " computing pmi ... 40000 words in 40069. mem= 3.284 Gb\n",
      "done\n",
      "....w2c_d300_w8_mf50_c200_pmi.csv created\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for es in word2concept_fname:\n",
    "    outputname=es.split(\"/\")[-1][:-4]+\"_pmi.csv\"\n",
    "    print(outputname)\n",
    "    concept_to_words = defaultdict(lambda: [])\n",
    "    with open(es, encoding='utf-8') as f:\n",
    "        for row in f:\n",
    "            cols = row.strip().split(',')\n",
    "            concept = int(cols[-1])\n",
    "            words = ','.join(cols[:-1])\n",
    "            concept_to_words[concept].append(words)\n",
    "    M_cooccurrence = defaultdict(lambda: {})\n",
    "    for concept, words in concept_to_words.items():\n",
    "        #print('concept= %d (%d words) ... ' % (concept, len(words)), end='')\n",
    "        for w1 in words:\n",
    "            for w2 in words:\n",
    "                if w1 <= w2: continue\n",
    "                cooc = cooccurrence(w1, w2)\n",
    "                M_cooccurrence[w1][w2] = cooc\n",
    "                M_cooccurrence[w2][w1] = cooc\n",
    "    print('done. mem= %.3f Gb' % get_process_memory())\n",
    "    word_to_pmi = {}\n",
    "\n",
    "    m = 1\n",
    "    n = 2\n",
    "\n",
    "    i_words = 0\n",
    "    n_words = sum((len(words) for words in concept_to_words.values()))\n",
    "\n",
    "    for concept, words in concept_to_words.items():\n",
    "        for word in words:\n",
    "            pmi=0\n",
    "            i_words += 1\n",
    "            if i_words % 100 == 0:\n",
    "                args = (i_words, n_words, get_process_memory())\n",
    "                sys.stdout.write('\\r computing pmi ... %d words in %d. mem= %.3f Gb' % args)\n",
    "\n",
    "            cooccurrence_vector = M_cooccurrence.get(word, {})\n",
    "\n",
    "            if not cooccurrence_vector:\n",
    "                continue\n",
    "            \n",
    "            for word2, cooc in cooccurrence_vector.items():\n",
    "                pmi+=math.log(((n_words*cooc)/(get_tf(word)*get_tf(word2)))+0.000001)\n",
    "            \n",
    "            word_to_pmi[word]=pmi\n",
    "            \n",
    "    print('\\ndone')\n",
    "    \n",
    "    with open(outputname, \"w\") as f:\n",
    "        for concept, words in concept_to_words.items():\n",
    "            topk_words = sorted(words, key=lambda x:word_to_pmi[x], reverse=True)\n",
    "            for w in topk_words:\n",
    "                f.write('%d, %s, %.3f, %d, %d\\n' % (concept, w, word_to_pmi[w], get_tf(w), get_df(w)))\n",
    "    print('....%s created' % outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
